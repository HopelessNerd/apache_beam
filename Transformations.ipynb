{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformations.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5hC07yChiE0S",
        "x9v2TBxOo4TH",
        "o5-6t-D4ZPqG",
        "8cg5t7p83zES",
        "WhDcCpRqd2qw",
        "1n5NH3e8jSoi",
        "SADV_PzUk5o-",
        "EwP2rIBok6f1",
        "VM6bNuha0gy2",
        "OFKRF4n-4wj7",
        "REuLoyy9e22D",
        "TsBpBtXRw178",
        "LL-soOKFylqr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqhUKVDfhabS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Installation\n",
        "Run the following command to install apache-beam\n",
        "\n",
        "Note: To run pipeline on the google colab environemnt, no need to install/configure runners. Each session in the colab is assigned with new virtual environment which forces us to install apache beam every time a new session is created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIOwZGFJhUE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!{'pip install apache-beam'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjAz8AKHherJ",
        "colab_type": "text"
      },
      "source": [
        "# Upload the required files\n",
        "\n",
        "All the files required to be consumed must be uploaded by the following command. Later transformations could be applied by reading the data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8udVrCR8hhjD",
        "colab_type": "code",
        "outputId": "4bc118cd-d362-4b8e-ecb5-d12104b836ec",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ffd45b6-9aff-4e48-87a5-83191ae642bb\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0ffd45b6-9aff-4e48-87a5-83191ae642bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dept_data.txt to dept_data.txt\n",
            "Saving location.txt to location.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hC07yChiE0S",
        "colab_type": "text"
      },
      "source": [
        "# Map method\n",
        "\n",
        "It takes the single output and returns the single output. For e.g. if a string is passed as input and a split is applied to the string. The Map method will return a list of splitted values.\n",
        "\n",
        "Following code demostrates the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9tKQ9bghkNI",
        "colab_type": "code",
        "outputId": "8839421e-8296-40b2-dbea-0e12b6171a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "def split_row(element):\n",
        "    return element.split(',')\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "    p1\n",
        "      |beam.io.ReadFromText('dept-data.txt')\n",
        "      \n",
        "      # Split row by columns/elements with comma and returns a list \n",
        "      |beam.Map(split_row) \n",
        "\n",
        "      |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9v2TBxOo4TH",
        "colab_type": "text"
      },
      "source": [
        "# Flat Map Method\n",
        "\n",
        "This method take a single input and returns multiple output unlike Map method. For e.g. If a string is passed as input Flat map will split the string with a separator and returns all separated individual values. \n",
        "\n",
        "Note: Flat map can explicitly return the single element by type casting the returning value to list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nf3vaEjpcS2",
        "colab_type": "code",
        "outputId": "93c0e5c9-70cf-47c8-b503-e13b3416300d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "    # Flat map returns individual elements as the output\n",
        "    |beam.FlatMap(lambda record: record.split(','))\n",
        "\n",
        "    # Returning value can be explicitly type casted to list for the single value\n",
        "    |beam.FlatMap(lambda record: [record.split(',')])\n",
        "    \n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "  \n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['149633CM']\n",
            "['Marco']\n",
            "['10']\n",
            "['Accounts']\n",
            "['1-01-2019']\n",
            "['212539MU']\n",
            "['Rebekah']\n",
            "['10']\n",
            "['Accounts']\n",
            "['1-01-2019']\n",
            "['231555ZZ']\n",
            "['Itoe']\n",
            "['10']\n",
            "['Accounts']\n",
            "['1-01-2019']\n",
            "['503996WI']\n",
            "['Edouard']\n",
            "['10']\n",
            "['Accounts']\n",
            "['1-01-2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5-6t-D4ZPqG",
        "colab_type": "text"
      },
      "source": [
        "# Filter Method\n",
        "\n",
        "This method filters the list based on the condition provided as the function. For e.g. If a list is passed to the lamda function and if it returns only the specific element of the list, it applies the same function to all the lists read from the source. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC2KK0alpv1m",
        "colab_type": "code",
        "outputId": "ec83ae07-aed1-4f1d-afd5-a901eb067fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "   p1\n",
        "    |beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "    # Map returns individual elements as the output\n",
        "    |beam.Map(lambda record: record.split(','))\n",
        "\n",
        "    # Returning value can be explicitly type casted to list for the single value\n",
        "    |beam.Filter(lambda record: record[3]=='Accounts')\n",
        "    \n",
        "    |beam.io.WriteToText('data/output_new_final')\n",
        "  \n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '2-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '2-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '2-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '2-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '2-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '2-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '2-01-2019']\n",
            "['718737IX', 'Ayumi', '10', 'Accounts', '2-01-2019']\n",
            "['149633CM', 'Marco', '10', 'Accounts', '3-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '3-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '3-01-2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cg5t7p83zES",
        "colab_type": "text"
      },
      "source": [
        "# Combine per key method\n",
        "\n",
        "This method performs the group by operation on the tuple with the same keys and apply the operation to each group specified in the argument.\n",
        "\n",
        "To demonstrate follow the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGFSKsdw4Qt0",
        "colab_type": "code",
        "outputId": "c5d33f35-a60f-4178-b94c-0b66d01951f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# Create the pipeline object \n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Input collection to read the data from the file and separate values by comma from each rows\n",
        "input_collection = ( \n",
        "                      p \n",
        "                      # When the file is read, it will separate file with each line\n",
        "                      | \"Read from the text file\" >> beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "                      # Each line needs to be separated by comma to perform further transformations\n",
        "                      | \"Split each rows with comma \" >> beam.Map(lambda element: element.split(','))\n",
        "                  \n",
        "                      # Retrieve all the records with the label 'Accounts' in the 4th column or Department column\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "                      # Assign each employee with the value 1 in a tuple\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "\n",
        "                      # Group all the tuples with the summation to count total number of employees\n",
        "                      | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "\n",
        "                      # Write the output to a file\n",
        "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "# Run the pipeline\n",
        "p.run()\n",
        "  \n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Account-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhDcCpRqd2qw",
        "colab_type": "text"
      },
      "source": [
        "# Label Transformations \n",
        "\n",
        "Each transformations can be uniquely labelled with a string. '>>' symbol is used to label each tranforms. Error will be thrown if there are any duplicate labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Q2aXrMakvr",
        "colab_type": "code",
        "outputId": "a85cc4c0-74b3-4407-9a1e-3600e0f5a340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "   p1\n",
        "    | 'Read from the file' >> beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "    # Map returns individual elements as the output\n",
        "    | 'Split each rows with comma' >> beam.Map(lambda record: record.split(','))\n",
        "    \n",
        "    | 'Write final output to file' >> beam.io.WriteToText('data/output_new_final')\n",
        "  \n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n5NH3e8jSoi",
        "colab_type": "text"
      },
      "source": [
        "# Using With Keyword\n",
        "\n",
        "If pipeline is needed to run within the scope, to look cleaner and neat, \"with\" statement can be used. Without with statement pipeline needed to be run by calling the run method. \n",
        "\n",
        "Following demonstrate the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLniuCiviSsL",
        "colab_type": "code",
        "outputId": "4911badb-52ff-489c-d0da-625dc9cdb1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# using with statement run method is not needed to be called\n",
        "with beam.Pipeline() as p1:\n",
        "\n",
        "  attendance_count = (\n",
        "    \n",
        "   p1\n",
        "    | 'Read from the file' >> beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "    # Map returns individual elements as the output\n",
        "    | 'Split each rows with comma' >> beam.Map(lambda record: record.split(','))\n",
        "    \n",
        "    | 'Write final output to file' >> beam.io.WriteToText('data/output_new_final')\n",
        "  \n",
        ")\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['149633CM', 'Marco', '10', 'Accounts', '1-01-2019']\n",
            "['212539MU', 'Rebekah', '10', 'Accounts', '1-01-2019']\n",
            "['231555ZZ', 'Itoe', '10', 'Accounts', '1-01-2019']\n",
            "['503996WI', 'Edouard', '10', 'Accounts', '1-01-2019']\n",
            "['704275DC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['957149WC', 'Kyle', '10', 'Accounts', '1-01-2019']\n",
            "['241316NX', 'Kumiko', '10', 'Accounts', '1-01-2019']\n",
            "['796656IE', 'Gaston', '10', 'Accounts', '1-01-2019']\n",
            "['331593PS', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['560447WH', 'Olga', '20', 'HR', '1-01-2019']\n",
            "['222997TJ', 'Leslie', '20', 'HR', '1-01-2019']\n",
            "['171752SY', 'Mindy', '20', 'HR', '1-01-2019']\n",
            "['153636AS', 'Vicky', '20', 'HR', '1-01-2019']\n",
            "['745411HT', 'Richard', '20', 'HR', '1-01-2019']\n",
            "['298464HN', 'Kirk', '20', 'HR', '1-01-2019']\n",
            "['783950BW', 'Kaori', '20', 'HR', '1-01-2019']\n",
            "['892691AR', 'Beryl', '20', 'HR', '1-01-2019']\n",
            "['245668UZ', 'Oscar', '20', 'HR', '1-01-2019']\n",
            "['231206QD', 'Kumiko', '30', 'Finance', '1-01-2019']\n",
            "['357919KT', 'Wendy', '30', 'Finance', '1-01-2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SADV_PzUk5o-",
        "colab_type": "text"
      },
      "source": [
        "# Branching Pipelines\n",
        "\n",
        "When there is a need to perform two or more different sets of tranforms from a single source for multiple outputs, branching pipelines come in handy. To branch a pipeline, multiple PCollections are created with different transforms in each collection. \n",
        "\n",
        "To demonstrate the pipelines with branches, follow the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ez0OQiMkarM",
        "colab_type": "code",
        "outputId": "d5674dbf-d09d-4162-b8f4-dfe7361ef3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# Create the pipeline object \n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Input collection to read the data from the file and separate values by comma from each rows\n",
        "input_collection = ( \n",
        "                      p \n",
        "                      # When the file is read, it will separate file with each line\n",
        "                      | \"Read from the text file\" >> beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "                      # Each line needs to be separated by comma to perform further transformations\n",
        "                      | \"Split each rows with comma \" >> beam.Map(lambda element: element.split(','))\n",
        "                   )\n",
        "\n",
        "# Count the number of employees in the accounts department and write results to a file\n",
        "accounts_count = (\n",
        "                      # Start the transformations on the results of the input_collection\n",
        "                      input_collection\n",
        "                  \n",
        "                      # Retrieve all the records with the label 'Accounts' in the 4th column or Department column\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "                      # Assign each employee with the value 1 in a tuple\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "\n",
        "                      # Group all the tuples with the summation to count total number of employees\n",
        "                      | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "\n",
        "                      # Write the output to a file\n",
        "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "# Count the number of employees in the HR department and write results to a file\n",
        "hr_count = (\n",
        "                # Start the transformations on the results of the input_collection\n",
        "                input_collection\n",
        "            \n",
        "                # Retrieve all the records with the label 'HR' in the 4th column or Department column\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "\n",
        "                # Assign each employee with the value 1 in a tuple\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "\n",
        "                # Group all the tuples with the summation to count total number of employees\n",
        "                | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "\n",
        "                # Write the output to a file\n",
        "                | 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
        "           )\n",
        "\n",
        "# Run the pipeline\n",
        "p.run()\n",
        "  \n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Account-00000-of-00001')}\n",
        "!{('head -n 20 data/HR-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n",
            "('HR, Beryl', 62)\n",
            "('HR, Olga', 31)\n",
            "('HR, Leslie', 31)\n",
            "('HR, Mindy', 31)\n",
            "('HR, Vicky', 31)\n",
            "('HR, Richard', 31)\n",
            "('HR, Kirk', 31)\n",
            "('HR, Kaori', 31)\n",
            "('HR, Oscar', 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwP2rIBok6f1",
        "colab_type": "text"
      },
      "source": [
        "# Flatten Transformation\n",
        "\n",
        "Two or more PCollections are created when the pipelines are branched. If results from different PCollection are to be merged, flatten transformation is used.\n",
        "Find the code below for the demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce4mdzaBzIRZ",
        "colab_type": "code",
        "outputId": "b2f4adf5-b903-4a2a-dd35-9e8a11259abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# Create the pipeline object \n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Input collection to read the data from the file and separate values by comma from each rows\n",
        "input_collection = ( \n",
        "                      p \n",
        "                      # When the file is read, it will separate file with each line\n",
        "                      | \"Read from the text file\" >> beam.io.ReadFromText('dept-data.txt')\n",
        "\n",
        "                      # Each line needs to be separated by comma to perform further transformations\n",
        "                      | \"Split each rows with comma \" >> beam.Map(lambda element: element.split(','))\n",
        "                   )\n",
        "\n",
        "# Count the number of employees in the accounts department and write results to a file\n",
        "accounts_count = (\n",
        "                      # Start the transformations on the results of the input_collection\n",
        "                      input_collection\n",
        "                  \n",
        "                      # Retrieve all the records with the label 'Accounts' in the 4th column or Department column\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "\n",
        "                      # Assign each employee with the value 1 in a tuple\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "\n",
        "                      # Group all the tuples with the summation to count total number of employees\n",
        "                      | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "\n",
        "                 )\n",
        "\n",
        "# Count the number of employees in the HR department and write results to a file\n",
        "hr_count = (\n",
        "                # Start the transformations on the results of the input_collection\n",
        "                input_collection\n",
        "            \n",
        "                # Retrieve all the records with the label 'HR' in the 4th column or Department column\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "\n",
        "                # Assign each employee with the value 1 in a tuple\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "\n",
        "                # Group all the tuples with the summation to count total number of employees\n",
        "                | 'Group and sum' >> beam.CombinePerKey(sum)\n",
        "           )\n",
        "\n",
        "# Merge/Flatten/Union the above two PCollections\n",
        "output =(\n",
        "                # Two collections in a tuple which are needed to be flattened \n",
        "                (accounts_count,hr_count)\n",
        "\n",
        "                # Flattens the provided PCollections\n",
        "                | beam.Flatten()\n",
        "\n",
        "                # Write the Output to the file\n",
        "                | beam.io.WriteToText('data/Both')\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "p.run()\n",
        "  \n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Both-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Accounts, Marco', 31)\n",
            "('Accounts, Rebekah', 31)\n",
            "('Accounts, Itoe', 31)\n",
            "('Accounts, Edouard', 31)\n",
            "('Accounts, Kyle', 62)\n",
            "('Accounts, Kumiko', 31)\n",
            "('Accounts, Gaston', 31)\n",
            "('Accounts, Ayumi', 30)\n",
            "('HR, Beryl', 62)\n",
            "('HR, Olga', 31)\n",
            "('HR, Leslie', 31)\n",
            "('HR, Mindy', 31)\n",
            "('HR, Vicky', 31)\n",
            "('HR, Richard', 31)\n",
            "('HR, Kirk', 31)\n",
            "('HR, Kaori', 31)\n",
            "('HR, Oscar', 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM6bNuha0gy2",
        "colab_type": "text"
      },
      "source": [
        "# Excersie: Word counts of the file\n",
        "\n",
        "Find the total number of words from the file. Try it on your own before moving to the solutions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RgGQ2BLz2H5",
        "colab_type": "code",
        "outputId": "c0bc7c6c-3509-4611-e759-dbf3d8735f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "    p1\n",
        "      # When the file is read, it will separate file with each line\n",
        "      |'Read from the file' >> beam.io.ReadFromText('dept-data.txt')\n",
        "      \n",
        "      # Split row by columns/elements with comma and returns a list \n",
        "      |'Split each row by comma' >> beam.FlatMap(lambda element: element.split(',')) \n",
        "\n",
        "      # Assign each word with a value 1\n",
        "      |'Assign a value 1 to each word' >> beam.Map(lambda element: (element,1))\n",
        "\n",
        "      # Apply sum to the grouped words\n",
        "      |'Group by words and apply sum to its value which is 1' >> beam.CombinePerKey(sum)\n",
        "\n",
        "      # Write the Output to the file\n",
        "      |'Write Output to a file' >> beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('149633CM', 31)\n",
            "('Marco', 31)\n",
            "('10', 278)\n",
            "('Accounts', 278)\n",
            "('1-01-2019', 28)\n",
            "('212539MU', 31)\n",
            "('Rebekah', 31)\n",
            "('231555ZZ', 31)\n",
            "('Itoe', 31)\n",
            "('503996WI', 31)\n",
            "('Edouard', 31)\n",
            "('704275DC', 31)\n",
            "('Kyle', 62)\n",
            "('957149WC', 31)\n",
            "('241316NX', 31)\n",
            "('Kumiko', 62)\n",
            "('796656IE', 31)\n",
            "('Gaston', 31)\n",
            "('331593PS', 31)\n",
            "('Beryl', 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFKRF4n-4wj7",
        "colab_type": "text"
      },
      "source": [
        "# ParDo Transform\n",
        "\n",
        "A ParDo transform takes each element of input PCollection, performs processing function on it and emits 0, 1 or multiple elements.\n",
        "\n",
        "Functionalities:\n",
        "> \n",
        "\t- Filtering\n",
        "\t\t○ ParDo can take each element of PCollection and decide either to output or discard it.\n",
        "\t- Formatting or Type conversion\n",
        "\t\t○ ParDo can change the type or format of input elements\n",
        "\t- Extracting individual parts\n",
        "\t\t○ ParDo can be used to extract individual elements from a single element\n",
        "\t- Computations\n",
        "\t\t○ ParDo can perform any processing function on the input elements and outputs a PCollection\n",
        "\n",
        "Note: Output could be emitted by using yield or return statement inside process method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3u6cU-2AcR",
        "colab_type": "code",
        "outputId": "4337c6d6-57ed-4601-80b1-2548cf60a556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# Extracting individual parts\n",
        "class SplitRow(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    return [element.split(',')]\n",
        "\n",
        "# Filtering\n",
        "class FilterColumn(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    if element[3]=='Accounts':\n",
        "      return [element]\n",
        "      \n",
        "# Formatting or Type conversion\n",
        "class ApplyValue(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    return [(element[3] + ': ' + element[1],1)]\n",
        "\n",
        "#Computations\n",
        "class ApplySummation(beam.DoFn):\n",
        "  def process(self, element):\n",
        "    (key, values) = element\n",
        "    return [(key, sum(values))]\n",
        "\n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "attendance_count = (\n",
        "    \n",
        "    p1\n",
        "      |beam.io.ReadFromText('dept-data.txt')\n",
        "      \n",
        "      # Split row by columns/elements with comma and returns a list \n",
        "      |beam.ParDo(SplitRow()) \n",
        "\n",
        "      # Lambda functions can also be used instead of creating a class\n",
        "      # |beam.ParDo(lambda element : [element.split(',')])\n",
        "\n",
        "      # Perform filter transforms using ParDo\n",
        "      |beam.ParDo(FilterColumn())\n",
        "\n",
        "      # Apply 1 to each elements \n",
        "      |beam.ParDo(ApplyValue())\n",
        "\n",
        "      # Apply Group by to combine the values in a list for all the same keys\n",
        "      |beam.GroupByKey()\n",
        "\n",
        "      # Apply summation to all the aggregated values\n",
        "      |beam.ParDo(ApplySummation())\n",
        "\n",
        "      # Write output to a file\n",
        "      |beam.io.WriteToText('data/output_new_final')\n",
        ")\n",
        "\n",
        "p1.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/output_new_final-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Accounts: Marco', 31)\n",
            "('Accounts: Rebekah', 31)\n",
            "('Accounts: Itoe', 31)\n",
            "('Accounts: Edouard', 31)\n",
            "('Accounts: Kyle', 62)\n",
            "('Accounts: Kumiko', 31)\n",
            "('Accounts: Gaston', 31)\n",
            "('Accounts: Ayumi', 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REuLoyy9e22D",
        "colab_type": "text"
      },
      "source": [
        "# Combine Transform\n",
        "Combiner is a mini reducer which does the reduce task locally to a mapper machine. It works for both associative and commutative nature. Combine has four following methods which needs to be overridden when inheriting the combine class\n",
        "\n",
        "\n",
        "\n",
        "1.   **Create Accumulator** creates a new “local” accumulator. In the example case, taking a mean average, a local accumulator tracks the running sum of values (the numerator value for our final average division) and the number of values summed so far (the denominator value). It may be called any number of times in a distributed fashion.\n",
        "2.   **Add Input** adds an input element to an accumulator, returning the accumulator value. In our example, it would update the sum and increment the count. It may also be invoked in parallel.\n",
        "3.   **Merge Accumulators** merges several accumulators into a single accumulator; this is how data in multiple accumulators is combined before the final calculation. In the case of the mean average computation, the accumulators representing each portion of the division are merged together. It may be called again on its outputs any number of times.\n",
        "4.   **Extract Output** performs the final computation. In the case of computing a mean average, this means dividing the combined sum of all the values by the number of values summed. It is called once on the final, merged accumulator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2eYvrAdLNJ_",
        "colab_type": "code",
        "outputId": "58699e73-039e-463a-8e51-0fe53f416627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "# Class inheriting the CombineFn class\n",
        "class AverageFn(beam.CombineFn):\n",
        "  \n",
        "  def create_accumulator(self):\n",
        "     return (0.0, 0)   # initialize (sum, count)\n",
        "\n",
        "  def add_input(self, sum_count, input):\n",
        "    (sum, count) = sum_count\n",
        "    return sum + input, count + 1\n",
        "\n",
        "  def merge_accumulators(self, accumulators):\n",
        "    \n",
        "    # * operator is an unpacking operator which helps zip function to unzip accumulators to sums and counts\n",
        "    # zip - [(27, 3), (39, 3), (18, 2)]  -->   [(27,39,18), (3,3,2)]\n",
        "    ind_sums, ind_counts = zip(*accumulators)      \n",
        "    return sum(ind_sums), sum(ind_counts)        # (84,8)\n",
        "\n",
        "  def extract_output(self, sum_count):    \n",
        "    \n",
        "    (sum, count) = sum_count    # combine globally using CombineFn\n",
        "    return sum / count if count else float('NaN')\n",
        "  \n",
        "\n",
        "small_sum = (\n",
        "           p \n",
        "            | beam.Create([15,5,7,7,9,23,13,5])\n",
        "            | \"Combine Globally\" >> beam.CombineGlobally(AverageFn()) \n",
        "            | 'Write results to a output file' >> beam.io.WriteToText('data/combine')\n",
        "          )\n",
        "p.run()\n",
        "\n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{'head -n 20 data/combine-00000-of-00001'}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsBpBtXRw178",
        "colab_type": "text"
      },
      "source": [
        "# Composite Transform:\n",
        "\n",
        "Transforms can have a nested structure, where a complex transform performs multiple simpler transforms (such as more than one ParDo, Combine, GroupByKey, or even other composite transforms). These transforms are called composite transforms. Nesting multiple transforms inside a single composite transform can make your code more modular and easier to understand.\n",
        "\n",
        "Following demonstrate composite transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iif5EK0tuM5q",
        "colab_type": "code",
        "outputId": "d9f56bd1-070e-4476-cf03-2a349bbdbe1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "class MyTransform(beam.PTransform):\n",
        "  \n",
        "  def expand(self, input_coll):\n",
        "    \n",
        "    a = ( \n",
        "        input_coll\n",
        "                       | 'Group and sum1' >> beam.CombinePerKey(sum)\n",
        "                       | 'count filter accounts' >> beam.Filter(filter_on_count)\n",
        "                       | 'Regular accounts employee' >> beam.Map(format_output)\n",
        "              \n",
        "    )\n",
        "    return a\n",
        "\n",
        "def SplitRow(element):\n",
        "    return element.split(',')\n",
        "  \n",
        "  \n",
        "def filter_on_count(element):\n",
        "  name, count = element\n",
        "  if count > 30:\n",
        "    return element\n",
        "  \n",
        "def format_output(element):\n",
        "  name, count = element\n",
        "  return ', '.join((str(count),'Regular employee'))\n",
        "\n",
        "p = beam.Pipeline()\n",
        "\n",
        "input_collection = ( \n",
        "                      p \n",
        "                      | \"Read from text file\" >> beam.io.ReadFromText('dept-data.txt')\n",
        "                      | \"Split rows\" >> beam.Map(SplitRow)\n",
        "                   )\n",
        "\n",
        "accounts_count = (\n",
        "                      input_collection\n",
        "                      | 'Get all Accounts dept persons' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
        "                      | 'Pair each accounts employee with 1' >> beam.Map(lambda record: (\"Accounts, \" +record[1], 1))\n",
        "                      | 'composite accounts' >> MyTransform()\n",
        "                      | 'Write results for account' >> beam.io.WriteToText('data/Account')\n",
        "                 )\n",
        "\n",
        "hr_count = (\n",
        "                input_collection\n",
        "                | 'Get all HR dept persons' >> beam.Filter(lambda record: record[3] == 'HR')\n",
        "                | 'Pair each hr employee with 1' >> beam.Map(lambda record: (\"HR, \" +record[1], 1))\n",
        "                | 'composite HR' >> MyTransform()\n",
        "                | 'Write results for hr' >> beam.io.WriteToText('data/HR')\n",
        "           ) \n",
        "p.run()\n",
        "  \n",
        "# Sample the first 20 results, remember there are no ordering guarantees.\n",
        "!{('head -n 20 data/Account-00000-of-00001')}\n",
        "!{('head -n 20 data/HR-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "62, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "62, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n",
            "31, Regular employee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL-soOKFylqr",
        "colab_type": "text"
      },
      "source": [
        "# CoGroupBy Transform\n",
        "\n",
        "CoGroupByKey performs a relational join of two or more key/value PCollections that have the same key type. Consider using CoGroupByKey if you have multiple data sets that provide information about related things. For example, let’s say you have two different files with user data: one file has names and email addresses; the other file has names and phone numbers. You can join those two data sets, using the user name as a common key and the other data as the associated values. After the join, you have one data set that contains all of the information (email addresses and phone numbers) associated with each name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acoTE_qkr8_6",
        "colab_type": "code",
        "outputId": "d8d98c7d-eb45-4cb9-de45-9d46a1ff39f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "# Create a tuple from the data having one key and rest columns as a value.\n",
        "def retTuple(element): \n",
        "\n",
        "  thisTuple=element.split(',')\n",
        "  return (thisTuple[0],thisTuple[1:])\n",
        "                \n",
        "p1 = beam.Pipeline()\n",
        "\n",
        "# Apply a ParDo to the PCollection \"words\" to compute lengths for each word.\n",
        "dep_rows = ( \n",
        "                p1\n",
        "                # Read data from the file\n",
        "                | \"Reading File 1\" >> beam.io.ReadFromText('dept_data.txt')\n",
        "                | 'Pair each employee with key' >> beam.Map(retTuple)          # {149633CM : [Marco,10,Accounts,1-01-2019]}\n",
        "    \n",
        "               )\n",
        "\n",
        "\n",
        "loc_rows = ( \n",
        "                p1\n",
        "                # Read data from the second file\n",
        "                | \"Reading File 2\" >> beam.io.ReadFromText('location.txt') \n",
        "                | 'Pair each loc with key' >> beam.Map(retTuple)                # {149633CM : [9876843261,New York]}\n",
        "               )\n",
        "\n",
        "\n",
        "results = ({'dep_data': dep_rows, 'loc_data': loc_rows} \n",
        "           \n",
        "           | beam.CoGroupByKey()\n",
        "           | 'Write results' >> beam.io.WriteToText('data/result')\n",
        "          )\n",
        "\n",
        "\n",
        "p1.run()\n",
        "\n",
        "!{('head -n 20 data/result-00000-of-00001')}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('149633CM', {'dep_data': [['Marco', '10', 'Accounts', '1-01-2019'], ['Marco', '10', 'Accounts', '2-01-2019'], ['Marco', '10', 'Accounts', '3-01-2019'], ['Marco', '10', 'Accounts', '4-01-2019'], ['Marco', '10', 'Accounts', '5-01-2019'], ['Marco', '10', 'Accounts', '6-01-2019'], ['Marco', '10', 'Accounts', '7-01-2019'], ['Marco', '10', 'Accounts', '8-01-2019'], ['Marco', '10', 'Accounts', '9-01-2019'], ['Marco', '10', 'Accounts', '10-01-2019'], ['Marco', '10', 'Accounts', '11-01-2019'], ['Marco', '10', 'Accounts', '12-01-2019'], ['Marco', '10', 'Accounts', '13-01-2019'], ['Marco', '10', 'Accounts', '14-01-2019'], ['Marco', '10', 'Accounts', '15-01-2019'], ['Marco', '10', 'Accounts', '16-01-2019'], ['Marco', '10', 'Accounts', '17-01-2019'], ['Marco', '10', 'Accounts', '18-01-2019'], ['Marco', '10', 'Accounts', '19-01-2019'], ['Marco', '10', 'Accounts', '20-01-2019'], ['Marco', '10', 'Accounts', '21-01-2019'], ['Marco', '10', 'Accounts', '22-01-2019'], ['Marco', '10', 'Accounts', '23-01-2019'], ['Marco', '10', 'Accounts', '24-01-2019'], ['Marco', '10', 'Accounts', '25-01-2019'], ['Marco', '10', 'Accounts', '26-01-2019'], ['Marco', '10', 'Accounts', '27-01-2019'], ['Marco', '10', 'Accounts', '28-01-2019'], ['Marco', '10', 'Accounts', '29-01-2019'], ['Marco', '10', 'Accounts', '30-01-2019'], ['Marco', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9876843261', 'New York'], ['9204232778', 'New York']]})\n",
            "('212539MU', {'dep_data': [['Rebekah', '10', 'Accounts', '1-01-2019'], ['Rebekah', '10', 'Accounts', '2-01-2019'], ['Rebekah', '10', 'Accounts', '3-01-2019'], ['Rebekah', '10', 'Accounts', '4-01-2019'], ['Rebekah', '10', 'Accounts', '5-01-2019'], ['Rebekah', '10', 'Accounts', '6-01-2019'], ['Rebekah', '10', 'Accounts', '7-01-2019'], ['Rebekah', '10', 'Accounts', '8-01-2019'], ['Rebekah', '10', 'Accounts', '9-01-2019'], ['Rebekah', '10', 'Accounts', '10-01-2019'], ['Rebekah', '10', 'Accounts', '11-01-2019'], ['Rebekah', '10', 'Accounts', '12-01-2019'], ['Rebekah', '10', 'Accounts', '13-01-2019'], ['Rebekah', '10', 'Accounts', '14-01-2019'], ['Rebekah', '10', 'Accounts', '15-01-2019'], ['Rebekah', '10', 'Accounts', '16-01-2019'], ['Rebekah', '10', 'Accounts', '17-01-2019'], ['Rebekah', '10', 'Accounts', '18-01-2019'], ['Rebekah', '10', 'Accounts', '19-01-2019'], ['Rebekah', '10', 'Accounts', '20-01-2019'], ['Rebekah', '10', 'Accounts', '21-01-2019'], ['Rebekah', '10', 'Accounts', '22-01-2019'], ['Rebekah', '10', 'Accounts', '23-01-2019'], ['Rebekah', '10', 'Accounts', '24-01-2019'], ['Rebekah', '10', 'Accounts', '25-01-2019'], ['Rebekah', '10', 'Accounts', '26-01-2019'], ['Rebekah', '10', 'Accounts', '27-01-2019'], ['Rebekah', '10', 'Accounts', '28-01-2019'], ['Rebekah', '10', 'Accounts', '29-01-2019'], ['Rebekah', '10', 'Accounts', '30-01-2019'], ['Rebekah', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9995440673', 'Denver']]})\n",
            "('231555ZZ', {'dep_data': [['Itoe', '10', 'Accounts', '1-01-2019'], ['Itoe', '10', 'Accounts', '2-01-2019'], ['Itoe', '10', 'Accounts', '3-01-2019'], ['Itoe', '10', 'Accounts', '4-01-2019'], ['Itoe', '10', 'Accounts', '5-01-2019'], ['Itoe', '10', 'Accounts', '6-01-2019'], ['Itoe', '10', 'Accounts', '7-01-2019'], ['Itoe', '10', 'Accounts', '8-01-2019'], ['Itoe', '10', 'Accounts', '9-01-2019'], ['Itoe', '10', 'Accounts', '10-01-2019'], ['Itoe', '10', 'Accounts', '11-01-2019'], ['Itoe', '10', 'Accounts', '12-01-2019'], ['Itoe', '10', 'Accounts', '13-01-2019'], ['Itoe', '10', 'Accounts', '14-01-2019'], ['Itoe', '10', 'Accounts', '15-01-2019'], ['Itoe', '10', 'Accounts', '16-01-2019'], ['Itoe', '10', 'Accounts', '17-01-2019'], ['Itoe', '10', 'Accounts', '18-01-2019'], ['Itoe', '10', 'Accounts', '19-01-2019'], ['Itoe', '10', 'Accounts', '20-01-2019'], ['Itoe', '10', 'Accounts', '21-01-2019'], ['Itoe', '10', 'Accounts', '22-01-2019'], ['Itoe', '10', 'Accounts', '23-01-2019'], ['Itoe', '10', 'Accounts', '24-01-2019'], ['Itoe', '10', 'Accounts', '25-01-2019'], ['Itoe', '10', 'Accounts', '26-01-2019'], ['Itoe', '10', 'Accounts', '27-01-2019'], ['Itoe', '10', 'Accounts', '28-01-2019'], ['Itoe', '10', 'Accounts', '29-01-2019'], ['Itoe', '10', 'Accounts', '30-01-2019'], ['Itoe', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9196597290', 'Boston']]})\n",
            "('503996WI', {'dep_data': [['Edouard', '10', 'Accounts', '1-01-2019'], ['Edouard', '10', 'Accounts', '2-01-2019'], ['Edouard', '10', 'Accounts', '3-01-2019'], ['Edouard', '10', 'Accounts', '4-01-2019'], ['Edouard', '10', 'Accounts', '5-01-2019'], ['Edouard', '10', 'Accounts', '6-01-2019'], ['Edouard', '10', 'Accounts', '7-01-2019'], ['Edouard', '10', 'Accounts', '8-01-2019'], ['Edouard', '10', 'Accounts', '9-01-2019'], ['Edouard', '10', 'Accounts', '10-01-2019'], ['Edouard', '10', 'Accounts', '11-01-2019'], ['Edouard', '10', 'Accounts', '12-01-2019'], ['Edouard', '10', 'Accounts', '13-01-2019'], ['Edouard', '10', 'Accounts', '14-01-2019'], ['Edouard', '10', 'Accounts', '15-01-2019'], ['Edouard', '10', 'Accounts', '16-01-2019'], ['Edouard', '10', 'Accounts', '17-01-2019'], ['Edouard', '10', 'Accounts', '18-01-2019'], ['Edouard', '10', 'Accounts', '19-01-2019'], ['Edouard', '10', 'Accounts', '20-01-2019'], ['Edouard', '10', 'Accounts', '21-01-2019'], ['Edouard', '10', 'Accounts', '22-01-2019'], ['Edouard', '10', 'Accounts', '23-01-2019'], ['Edouard', '10', 'Accounts', '24-01-2019'], ['Edouard', '10', 'Accounts', '25-01-2019'], ['Edouard', '10', 'Accounts', '26-01-2019'], ['Edouard', '10', 'Accounts', '27-01-2019'], ['Edouard', '10', 'Accounts', '28-01-2019'], ['Edouard', '10', 'Accounts', '29-01-2019'], ['Edouard', '10', 'Accounts', '30-01-2019'], ['Edouard', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9468234252', 'Miami']]})\n",
            "('704275DC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9776235961', 'Miami']]})\n",
            "('957149WC', {'dep_data': [['Kyle', '10', 'Accounts', '1-01-2019'], ['Kyle', '10', 'Accounts', '2-01-2019'], ['Kyle', '10', 'Accounts', '3-01-2019'], ['Kyle', '10', 'Accounts', '4-01-2019'], ['Kyle', '10', 'Accounts', '5-01-2019'], ['Kyle', '10', 'Accounts', '6-01-2019'], ['Kyle', '10', 'Accounts', '7-01-2019'], ['Kyle', '10', 'Accounts', '8-01-2019'], ['Kyle', '10', 'Accounts', '9-01-2019'], ['Kyle', '10', 'Accounts', '10-01-2019'], ['Kyle', '10', 'Accounts', '11-01-2019'], ['Kyle', '10', 'Accounts', '12-01-2019'], ['Kyle', '10', 'Accounts', '13-01-2019'], ['Kyle', '10', 'Accounts', '14-01-2019'], ['Kyle', '10', 'Accounts', '15-01-2019'], ['Kyle', '10', 'Accounts', '16-01-2019'], ['Kyle', '10', 'Accounts', '17-01-2019'], ['Kyle', '10', 'Accounts', '18-01-2019'], ['Kyle', '10', 'Accounts', '19-01-2019'], ['Kyle', '10', 'Accounts', '20-01-2019'], ['Kyle', '10', 'Accounts', '21-01-2019'], ['Kyle', '10', 'Accounts', '22-01-2019'], ['Kyle', '10', 'Accounts', '23-01-2019'], ['Kyle', '10', 'Accounts', '24-01-2019'], ['Kyle', '10', 'Accounts', '25-01-2019'], ['Kyle', '10', 'Accounts', '26-01-2019'], ['Kyle', '10', 'Accounts', '27-01-2019'], ['Kyle', '10', 'Accounts', '28-01-2019'], ['Kyle', '10', 'Accounts', '29-01-2019'], ['Kyle', '10', 'Accounts', '30-01-2019'], ['Kyle', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9925595092', 'Houston']]})\n",
            "('241316NX', {'dep_data': [['Kumiko', '10', 'Accounts', '1-01-2019'], ['Kumiko', '10', 'Accounts', '2-01-2019'], ['Kumiko', '10', 'Accounts', '3-01-2019'], ['Kumiko', '10', 'Accounts', '4-01-2019'], ['Kumiko', '10', 'Accounts', '5-01-2019'], ['Kumiko', '10', 'Accounts', '6-01-2019'], ['Kumiko', '10', 'Accounts', '7-01-2019'], ['Kumiko', '10', 'Accounts', '8-01-2019'], ['Kumiko', '10', 'Accounts', '9-01-2019'], ['Kumiko', '10', 'Accounts', '10-01-2019'], ['Kumiko', '10', 'Accounts', '11-01-2019'], ['Kumiko', '10', 'Accounts', '12-01-2019'], ['Kumiko', '10', 'Accounts', '13-01-2019'], ['Kumiko', '10', 'Accounts', '14-01-2019'], ['Kumiko', '10', 'Accounts', '15-01-2019'], ['Kumiko', '10', 'Accounts', '16-01-2019'], ['Kumiko', '10', 'Accounts', '17-01-2019'], ['Kumiko', '10', 'Accounts', '18-01-2019'], ['Kumiko', '10', 'Accounts', '19-01-2019'], ['Kumiko', '10', 'Accounts', '20-01-2019'], ['Kumiko', '10', 'Accounts', '21-01-2019'], ['Kumiko', '10', 'Accounts', '22-01-2019'], ['Kumiko', '10', 'Accounts', '23-01-2019'], ['Kumiko', '10', 'Accounts', '24-01-2019'], ['Kumiko', '10', 'Accounts', '25-01-2019'], ['Kumiko', '10', 'Accounts', '26-01-2019'], ['Kumiko', '10', 'Accounts', '27-01-2019'], ['Kumiko', '10', 'Accounts', '28-01-2019'], ['Kumiko', '10', 'Accounts', '29-01-2019'], ['Kumiko', '10', 'Accounts', '30-01-2019'], ['Kumiko', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9837402343', 'Boston']]})\n",
            "('796656IE', {'dep_data': [['Gaston', '10', 'Accounts', '1-01-2019'], ['Gaston', '10', 'Accounts', '2-01-2019'], ['Gaston', '10', 'Accounts', '3-01-2019'], ['Gaston', '10', 'Accounts', '4-01-2019'], ['Gaston', '10', 'Accounts', '5-01-2019'], ['Gaston', '10', 'Accounts', '6-01-2019'], ['Gaston', '10', 'Accounts', '7-01-2019'], ['Gaston', '10', 'Accounts', '8-01-2019'], ['Gaston', '10', 'Accounts', '9-01-2019'], ['Gaston', '10', 'Accounts', '10-01-2019'], ['Gaston', '10', 'Accounts', '11-01-2019'], ['Gaston', '10', 'Accounts', '12-01-2019'], ['Gaston', '10', 'Accounts', '13-01-2019'], ['Gaston', '10', 'Accounts', '14-01-2019'], ['Gaston', '10', 'Accounts', '15-01-2019'], ['Gaston', '10', 'Accounts', '16-01-2019'], ['Gaston', '10', 'Accounts', '17-01-2019'], ['Gaston', '10', 'Accounts', '18-01-2019'], ['Gaston', '10', 'Accounts', '19-01-2019'], ['Gaston', '10', 'Accounts', '20-01-2019'], ['Gaston', '10', 'Accounts', '21-01-2019'], ['Gaston', '10', 'Accounts', '22-01-2019'], ['Gaston', '10', 'Accounts', '23-01-2019'], ['Gaston', '10', 'Accounts', '24-01-2019'], ['Gaston', '10', 'Accounts', '25-01-2019'], ['Gaston', '10', 'Accounts', '26-01-2019'], ['Gaston', '10', 'Accounts', '27-01-2019'], ['Gaston', '10', 'Accounts', '28-01-2019'], ['Gaston', '10', 'Accounts', '29-01-2019'], ['Gaston', '10', 'Accounts', '30-01-2019'], ['Gaston', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9538848876', 'Houston']]})\n",
            "('718737IX', {'dep_data': [['Ayumi', '10', 'Accounts', '2-01-2019'], ['Ayumi', '10', 'Accounts', '3-01-2019'], ['Ayumi', '10', 'Accounts', '4-01-2019'], ['Ayumi', '10', 'Accounts', '5-01-2019'], ['Ayumi', '10', 'Accounts', '6-01-2019'], ['Ayumi', '10', 'Accounts', '7-01-2019'], ['Ayumi', '10', 'Accounts', '8-01-2019'], ['Ayumi', '10', 'Accounts', '9-01-2019'], ['Ayumi', '10', 'Accounts', '10-01-2019'], ['Ayumi', '10', 'Accounts', '11-01-2019'], ['Ayumi', '10', 'Accounts', '12-01-2019'], ['Ayumi', '10', 'Accounts', '13-01-2019'], ['Ayumi', '10', 'Accounts', '14-01-2019'], ['Ayumi', '10', 'Accounts', '15-01-2019'], ['Ayumi', '10', 'Accounts', '16-01-2019'], ['Ayumi', '10', 'Accounts', '17-01-2019'], ['Ayumi', '10', 'Accounts', '18-01-2019'], ['Ayumi', '10', 'Accounts', '19-01-2019'], ['Ayumi', '10', 'Accounts', '20-01-2019'], ['Ayumi', '10', 'Accounts', '21-01-2019'], ['Ayumi', '10', 'Accounts', '22-01-2019'], ['Ayumi', '10', 'Accounts', '23-01-2019'], ['Ayumi', '10', 'Accounts', '24-01-2019'], ['Ayumi', '10', 'Accounts', '25-01-2019'], ['Ayumi', '10', 'Accounts', '26-01-2019'], ['Ayumi', '10', 'Accounts', '27-01-2019'], ['Ayumi', '10', 'Accounts', '28-01-2019'], ['Ayumi', '10', 'Accounts', '29-01-2019'], ['Ayumi', '10', 'Accounts', '30-01-2019'], ['Ayumi', '10', 'Accounts', '31-01-2019']], 'loc_data': [['9204232788', 'Austin']]})\n",
            "('331593PS', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9137216186', 'Miami']]})\n",
            "('560447WH', {'dep_data': [['Olga', '20', 'HR', '1-01-2019'], ['Olga', '20', 'HR', '2-01-2019'], ['Olga', '20', 'HR', '3-01-2019'], ['Olga', '20', 'HR', '4-01-2019'], ['Olga', '20', 'HR', '5-01-2019'], ['Olga', '20', 'HR', '6-01-2019'], ['Olga', '20', 'HR', '7-01-2019'], ['Olga', '20', 'HR', '8-01-2019'], ['Olga', '20', 'HR', '9-01-2019'], ['Olga', '20', 'HR', '10-01-2019'], ['Olga', '20', 'HR', '11-01-2019'], ['Olga', '20', 'HR', '12-01-2019'], ['Olga', '20', 'HR', '13-01-2019'], ['Olga', '20', 'HR', '14-01-2019'], ['Olga', '20', 'HR', '15-01-2019'], ['Olga', '20', 'HR', '16-01-2019'], ['Olga', '20', 'HR', '17-01-2019'], ['Olga', '20', 'HR', '18-01-2019'], ['Olga', '20', 'HR', '19-01-2019'], ['Olga', '20', 'HR', '20-01-2019'], ['Olga', '20', 'HR', '21-01-2019'], ['Olga', '20', 'HR', '22-01-2019'], ['Olga', '20', 'HR', '23-01-2019'], ['Olga', '20', 'HR', '24-01-2019'], ['Olga', '20', 'HR', '25-01-2019'], ['Olga', '20', 'HR', '26-01-2019'], ['Olga', '20', 'HR', '27-01-2019'], ['Olga', '20', 'HR', '28-01-2019'], ['Olga', '20', 'HR', '29-01-2019'], ['Olga', '20', 'HR', '30-01-2019'], ['Olga', '20', 'HR', '31-01-2019']], 'loc_data': [['9708010864', 'Miami']]})\n",
            "('222997TJ', {'dep_data': [['Leslie', '20', 'HR', '1-01-2019'], ['Leslie', '20', 'HR', '2-01-2019'], ['Leslie', '20', 'HR', '3-01-2019'], ['Leslie', '20', 'HR', '4-01-2019'], ['Leslie', '20', 'HR', '5-01-2019'], ['Leslie', '20', 'HR', '6-01-2019'], ['Leslie', '20', 'HR', '7-01-2019'], ['Leslie', '20', 'HR', '8-01-2019'], ['Leslie', '20', 'HR', '9-01-2019'], ['Leslie', '20', 'HR', '10-01-2019'], ['Leslie', '20', 'HR', '11-01-2019'], ['Leslie', '20', 'HR', '12-01-2019'], ['Leslie', '20', 'HR', '13-01-2019'], ['Leslie', '20', 'HR', '14-01-2019'], ['Leslie', '20', 'HR', '15-01-2019'], ['Leslie', '20', 'HR', '16-01-2019'], ['Leslie', '20', 'HR', '17-01-2019'], ['Leslie', '20', 'HR', '18-01-2019'], ['Leslie', '20', 'HR', '19-01-2019'], ['Leslie', '20', 'HR', '20-01-2019'], ['Leslie', '20', 'HR', '21-01-2019'], ['Leslie', '20', 'HR', '22-01-2019'], ['Leslie', '20', 'HR', '23-01-2019'], ['Leslie', '20', 'HR', '24-01-2019'], ['Leslie', '20', 'HR', '25-01-2019'], ['Leslie', '20', 'HR', '26-01-2019'], ['Leslie', '20', 'HR', '27-01-2019'], ['Leslie', '20', 'HR', '28-01-2019'], ['Leslie', '20', 'HR', '29-01-2019'], ['Leslie', '20', 'HR', '30-01-2019'], ['Leslie', '20', 'HR', '31-01-2019']], 'loc_data': [['9410006713', 'Washington']]})\n",
            "('171752SY', {'dep_data': [['Mindy', '20', 'HR', '1-01-2019'], ['Mindy', '20', 'HR', '2-01-2019'], ['Mindy', '20', 'HR', '3-01-2019'], ['Mindy', '20', 'HR', '4-01-2019'], ['Mindy', '20', 'HR', '5-01-2019'], ['Mindy', '20', 'HR', '6-01-2019'], ['Mindy', '20', 'HR', '7-01-2019'], ['Mindy', '20', 'HR', '8-01-2019'], ['Mindy', '20', 'HR', '9-01-2019'], ['Mindy', '20', 'HR', '10-01-2019'], ['Mindy', '20', 'HR', '11-01-2019'], ['Mindy', '20', 'HR', '12-01-2019'], ['Mindy', '20', 'HR', '13-01-2019'], ['Mindy', '20', 'HR', '14-01-2019'], ['Mindy', '20', 'HR', '15-01-2019'], ['Mindy', '20', 'HR', '16-01-2019'], ['Mindy', '20', 'HR', '17-01-2019'], ['Mindy', '20', 'HR', '18-01-2019'], ['Mindy', '20', 'HR', '19-01-2019'], ['Mindy', '20', 'HR', '20-01-2019'], ['Mindy', '20', 'HR', '21-01-2019'], ['Mindy', '20', 'HR', '22-01-2019'], ['Mindy', '20', 'HR', '23-01-2019'], ['Mindy', '20', 'HR', '24-01-2019'], ['Mindy', '20', 'HR', '25-01-2019'], ['Mindy', '20', 'HR', '26-01-2019'], ['Mindy', '20', 'HR', '27-01-2019'], ['Mindy', '20', 'HR', '28-01-2019'], ['Mindy', '20', 'HR', '29-01-2019'], ['Mindy', '20', 'HR', '30-01-2019'], ['Mindy', '20', 'HR', '31-01-2019']], 'loc_data': [['9494683837', 'Boston']]})\n",
            "('153636AS', {'dep_data': [['Vicky', '20', 'HR', '1-01-2019'], ['Vicky', '20', 'HR', '2-01-2019'], ['Vicky', '20', 'HR', '3-01-2019'], ['Vicky', '20', 'HR', '4-01-2019'], ['Vicky', '20', 'HR', '5-01-2019'], ['Vicky', '20', 'HR', '6-01-2019'], ['Vicky', '20', 'HR', '7-01-2019'], ['Vicky', '20', 'HR', '8-01-2019'], ['Vicky', '20', 'HR', '9-01-2019'], ['Vicky', '20', 'HR', '10-01-2019'], ['Vicky', '20', 'HR', '11-01-2019'], ['Vicky', '20', 'HR', '12-01-2019'], ['Vicky', '20', 'HR', '13-01-2019'], ['Vicky', '20', 'HR', '14-01-2019'], ['Vicky', '20', 'HR', '15-01-2019'], ['Vicky', '20', 'HR', '16-01-2019'], ['Vicky', '20', 'HR', '17-01-2019'], ['Vicky', '20', 'HR', '18-01-2019'], ['Vicky', '20', 'HR', '19-01-2019'], ['Vicky', '20', 'HR', '20-01-2019'], ['Vicky', '20', 'HR', '21-01-2019'], ['Vicky', '20', 'HR', '22-01-2019'], ['Vicky', '20', 'HR', '23-01-2019'], ['Vicky', '20', 'HR', '24-01-2019'], ['Vicky', '20', 'HR', '25-01-2019'], ['Vicky', '20', 'HR', '26-01-2019'], ['Vicky', '20', 'HR', '27-01-2019'], ['Vicky', '20', 'HR', '28-01-2019'], ['Vicky', '20', 'HR', '29-01-2019'], ['Vicky', '20', 'HR', '30-01-2019'], ['Vicky', '20', 'HR', '31-01-2019']], 'loc_data': [['9575378417', 'New York']]})\n",
            "('745411HT', {'dep_data': [['Richard', '20', 'HR', '1-01-2019'], ['Richard', '20', 'HR', '2-01-2019'], ['Richard', '20', 'HR', '3-01-2019'], ['Richard', '20', 'HR', '4-01-2019'], ['Richard', '20', 'HR', '5-01-2019'], ['Richard', '20', 'HR', '6-01-2019'], ['Richard', '20', 'HR', '7-01-2019'], ['Richard', '20', 'HR', '8-01-2019'], ['Richard', '20', 'HR', '9-01-2019'], ['Richard', '20', 'HR', '10-01-2019'], ['Richard', '20', 'HR', '11-01-2019'], ['Richard', '20', 'HR', '12-01-2019'], ['Richard', '20', 'HR', '13-01-2019'], ['Richard', '20', 'HR', '14-01-2019'], ['Richard', '20', 'HR', '15-01-2019'], ['Richard', '20', 'HR', '16-01-2019'], ['Richard', '20', 'HR', '17-01-2019'], ['Richard', '20', 'HR', '18-01-2019'], ['Richard', '20', 'HR', '19-01-2019'], ['Richard', '20', 'HR', '20-01-2019'], ['Richard', '20', 'HR', '21-01-2019'], ['Richard', '20', 'HR', '22-01-2019'], ['Richard', '20', 'HR', '23-01-2019'], ['Richard', '20', 'HR', '24-01-2019'], ['Richard', '20', 'HR', '25-01-2019'], ['Richard', '20', 'HR', '26-01-2019'], ['Richard', '20', 'HR', '27-01-2019'], ['Richard', '20', 'HR', '28-01-2019'], ['Richard', '20', 'HR', '29-01-2019'], ['Richard', '20', 'HR', '30-01-2019'], ['Richard', '20', 'HR', '31-01-2019']], 'loc_data': [['9391632080', 'New York']]})\n",
            "('298464HN', {'dep_data': [['Kirk', '20', 'HR', '1-01-2019'], ['Kirk', '20', 'HR', '2-01-2019'], ['Kirk', '20', 'HR', '3-01-2019'], ['Kirk', '20', 'HR', '4-01-2019'], ['Kirk', '20', 'HR', '5-01-2019'], ['Kirk', '20', 'HR', '6-01-2019'], ['Kirk', '20', 'HR', '7-01-2019'], ['Kirk', '20', 'HR', '8-01-2019'], ['Kirk', '20', 'HR', '9-01-2019'], ['Kirk', '20', 'HR', '10-01-2019'], ['Kirk', '20', 'HR', '11-01-2019'], ['Kirk', '20', 'HR', '12-01-2019'], ['Kirk', '20', 'HR', '13-01-2019'], ['Kirk', '20', 'HR', '14-01-2019'], ['Kirk', '20', 'HR', '15-01-2019'], ['Kirk', '20', 'HR', '16-01-2019'], ['Kirk', '20', 'HR', '17-01-2019'], ['Kirk', '20', 'HR', '18-01-2019'], ['Kirk', '20', 'HR', '19-01-2019'], ['Kirk', '20', 'HR', '20-01-2019'], ['Kirk', '20', 'HR', '21-01-2019'], ['Kirk', '20', 'HR', '22-01-2019'], ['Kirk', '20', 'HR', '23-01-2019'], ['Kirk', '20', 'HR', '24-01-2019'], ['Kirk', '20', 'HR', '25-01-2019'], ['Kirk', '20', 'HR', '26-01-2019'], ['Kirk', '20', 'HR', '27-01-2019'], ['Kirk', '20', 'HR', '28-01-2019'], ['Kirk', '20', 'HR', '29-01-2019'], ['Kirk', '20', 'HR', '30-01-2019'], ['Kirk', '20', 'HR', '31-01-2019']], 'loc_data': [['9982641601', 'Miami']]})\n",
            "('783950BW', {'dep_data': [['Kaori', '20', 'HR', '1-01-2019'], ['Kaori', '20', 'HR', '2-01-2019'], ['Kaori', '20', 'HR', '3-01-2019'], ['Kaori', '20', 'HR', '4-01-2019'], ['Kaori', '20', 'HR', '5-01-2019'], ['Kaori', '20', 'HR', '6-01-2019'], ['Kaori', '20', 'HR', '7-01-2019'], ['Kaori', '20', 'HR', '8-01-2019'], ['Kaori', '20', 'HR', '9-01-2019'], ['Kaori', '20', 'HR', '10-01-2019'], ['Kaori', '20', 'HR', '11-01-2019'], ['Kaori', '20', 'HR', '12-01-2019'], ['Kaori', '20', 'HR', '13-01-2019'], ['Kaori', '20', 'HR', '14-01-2019'], ['Kaori', '20', 'HR', '15-01-2019'], ['Kaori', '20', 'HR', '16-01-2019'], ['Kaori', '20', 'HR', '17-01-2019'], ['Kaori', '20', 'HR', '18-01-2019'], ['Kaori', '20', 'HR', '19-01-2019'], ['Kaori', '20', 'HR', '20-01-2019'], ['Kaori', '20', 'HR', '21-01-2019'], ['Kaori', '20', 'HR', '22-01-2019'], ['Kaori', '20', 'HR', '23-01-2019'], ['Kaori', '20', 'HR', '24-01-2019'], ['Kaori', '20', 'HR', '25-01-2019'], ['Kaori', '20', 'HR', '26-01-2019'], ['Kaori', '20', 'HR', '27-01-2019'], ['Kaori', '20', 'HR', '28-01-2019'], ['Kaori', '20', 'HR', '29-01-2019'], ['Kaori', '20', 'HR', '30-01-2019'], ['Kaori', '20', 'HR', '31-01-2019']], 'loc_data': [['9248480224', 'Boston']]})\n",
            "('892691AR', {'dep_data': [['Beryl', '20', 'HR', '1-01-2019'], ['Beryl', '20', 'HR', '2-01-2019'], ['Beryl', '20', 'HR', '3-01-2019'], ['Beryl', '20', 'HR', '4-01-2019'], ['Beryl', '20', 'HR', '5-01-2019'], ['Beryl', '20', 'HR', '6-01-2019'], ['Beryl', '20', 'HR', '7-01-2019'], ['Beryl', '20', 'HR', '8-01-2019'], ['Beryl', '20', 'HR', '9-01-2019'], ['Beryl', '20', 'HR', '10-01-2019'], ['Beryl', '20', 'HR', '11-01-2019'], ['Beryl', '20', 'HR', '12-01-2019'], ['Beryl', '20', 'HR', '13-01-2019'], ['Beryl', '20', 'HR', '14-01-2019'], ['Beryl', '20', 'HR', '15-01-2019'], ['Beryl', '20', 'HR', '16-01-2019'], ['Beryl', '20', 'HR', '17-01-2019'], ['Beryl', '20', 'HR', '18-01-2019'], ['Beryl', '20', 'HR', '19-01-2019'], ['Beryl', '20', 'HR', '20-01-2019'], ['Beryl', '20', 'HR', '21-01-2019'], ['Beryl', '20', 'HR', '22-01-2019'], ['Beryl', '20', 'HR', '23-01-2019'], ['Beryl', '20', 'HR', '24-01-2019'], ['Beryl', '20', 'HR', '25-01-2019'], ['Beryl', '20', 'HR', '26-01-2019'], ['Beryl', '20', 'HR', '27-01-2019'], ['Beryl', '20', 'HR', '28-01-2019'], ['Beryl', '20', 'HR', '29-01-2019'], ['Beryl', '20', 'HR', '30-01-2019'], ['Beryl', '20', 'HR', '31-01-2019']], 'loc_data': [['9723803710', 'New York']]})\n",
            "('245668UZ', {'dep_data': [['Oscar', '20', 'HR', '1-01-2019'], ['Oscar', '20', 'HR', '2-01-2019'], ['Oscar', '20', 'HR', '3-01-2019'], ['Oscar', '20', 'HR', '4-01-2019'], ['Oscar', '20', 'HR', '5-01-2019'], ['Oscar', '20', 'HR', '6-01-2019'], ['Oscar', '20', 'HR', '7-01-2019'], ['Oscar', '20', 'HR', '8-01-2019'], ['Oscar', '20', 'HR', '9-01-2019'], ['Oscar', '20', 'HR', '10-01-2019'], ['Oscar', '20', 'HR', '11-01-2019'], ['Oscar', '20', 'HR', '12-01-2019'], ['Oscar', '20', 'HR', '13-01-2019'], ['Oscar', '20', 'HR', '14-01-2019'], ['Oscar', '20', 'HR', '15-01-2019'], ['Oscar', '20', 'HR', '16-01-2019'], ['Oscar', '20', 'HR', '17-01-2019'], ['Oscar', '20', 'HR', '18-01-2019'], ['Oscar', '20', 'HR', '19-01-2019'], ['Oscar', '20', 'HR', '20-01-2019'], ['Oscar', '20', 'HR', '21-01-2019'], ['Oscar', '20', 'HR', '22-01-2019'], ['Oscar', '20', 'HR', '23-01-2019'], ['Oscar', '20', 'HR', '24-01-2019'], ['Oscar', '20', 'HR', '25-01-2019'], ['Oscar', '20', 'HR', '26-01-2019'], ['Oscar', '20', 'HR', '27-01-2019'], ['Oscar', '20', 'HR', '28-01-2019'], ['Oscar', '20', 'HR', '29-01-2019'], ['Oscar', '20', 'HR', '30-01-2019'], ['Oscar', '20', 'HR', '31-01-2019']], 'loc_data': [['9395037841', 'New York']]})\n",
            "('231206QD', {'dep_data': [['Kumiko', '30', 'Finance', '1-01-2019'], ['Kumiko', '30', 'Finance', '2-01-2019'], ['Kumiko', '30', 'Finance', '3-01-2019'], ['Kumiko', '30', 'Finance', '4-01-2019'], ['Kumiko', '30', 'Finance', '5-01-2019'], ['Kumiko', '30', 'Finance', '6-01-2019'], ['Kumiko', '30', 'Finance', '7-01-2019'], ['Kumiko', '30', 'Finance', '8-01-2019'], ['Kumiko', '30', 'Finance', '9-01-2019'], ['Kumiko', '30', 'Finance', '10-01-2019'], ['Kumiko', '30', 'Finance', '11-01-2019'], ['Kumiko', '30', 'Finance', '12-01-2019'], ['Kumiko', '30', 'Finance', '13-01-2019'], ['Kumiko', '30', 'Finance', '14-01-2019'], ['Kumiko', '30', 'Finance', '15-01-2019'], ['Kumiko', '30', 'Finance', '16-01-2019'], ['Kumiko', '30', 'Finance', '17-01-2019'], ['Kumiko', '30', 'Finance', '18-01-2019'], ['Kumiko', '30', 'Finance', '19-01-2019'], ['Kumiko', '30', 'Finance', '20-01-2019'], ['Kumiko', '30', 'Finance', '21-01-2019'], ['Kumiko', '30', 'Finance', '22-01-2019'], ['Kumiko', '30', 'Finance', '23-01-2019'], ['Kumiko', '30', 'Finance', '24-01-2019'], ['Kumiko', '30', 'Finance', '25-01-2019'], ['Kumiko', '30', 'Finance', '26-01-2019'], ['Kumiko', '30', 'Finance', '27-01-2019'], ['Kumiko', '30', 'Finance', '28-01-2019'], ['Kumiko', '30', 'Finance', '29-01-2019'], ['Kumiko', '30', 'Finance', '30-01-2019'], ['Kumiko', '30', 'Finance', '31-01-2019']], 'loc_data': [['9608996582', 'Chicago']]})\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}